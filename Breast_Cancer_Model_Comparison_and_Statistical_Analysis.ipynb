{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf53240-bea2-4a1c-af83-8c38ddf7ee17",
   "metadata": {},
   "source": [
    "# Model Comparison and A/B Testing\n",
    "\n",
    "In this notebook, we will:\n",
    "- Compare the performance of Logistic Regression, SVM, and Random Forest on a classification task.\n",
    "- Evaluate the models using accuracy, classification reports, and confusion matrices.\n",
    "- Perform hyperparameter tuning to improve model performance.\n",
    "- Conduct A/B testing between the models with and without hyperparameter tuning to assess if hyperparameter tuning leads to significantly better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2667c2ce-1819-4c3e-af4d-9c00be12f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c94e7a-4f0a-48f3-acef-8fa748300478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n",
      "None\n",
      "\n",
      "First 5 Rows of the Dataset:\n",
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0  ...          17.33           184.60      2019.0            0.1622   \n",
      "1  ...          23.41           158.80      1956.0            0.1238   \n",
      "2  ...          25.53           152.50      1709.0            0.1444   \n",
      "3  ...          26.50            98.87       567.7            0.2098   \n",
      "4  ...          16.67           152.20      1575.0            0.1374   \n",
      "\n",
      "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0             0.6656           0.7119                0.2654          0.4601   \n",
      "1             0.1866           0.2416                0.1860          0.2750   \n",
      "2             0.4245           0.4504                0.2430          0.3613   \n",
      "3             0.8663           0.6869                0.2575          0.6638   \n",
      "4             0.2050           0.4000                0.1625          0.2364   \n",
      "\n",
      "   fractal_dimension_worst  Unnamed: 32  \n",
      "0                  0.11890          NaN  \n",
      "1                  0.08902          NaN  \n",
      "2                  0.08758          NaN  \n",
      "3                  0.17300          NaN  \n",
      "4                  0.07678          NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "Missing Values in the Dataset:\n",
      "id                           0\n",
      "diagnosis                    0\n",
      "radius_mean                  0\n",
      "texture_mean                 0\n",
      "perimeter_mean               0\n",
      "area_mean                    0\n",
      "smoothness_mean              0\n",
      "compactness_mean             0\n",
      "concavity_mean               0\n",
      "concave points_mean          0\n",
      "symmetry_mean                0\n",
      "fractal_dimension_mean       0\n",
      "radius_se                    0\n",
      "texture_se                   0\n",
      "perimeter_se                 0\n",
      "area_se                      0\n",
      "smoothness_se                0\n",
      "compactness_se               0\n",
      "concavity_se                 0\n",
      "concave points_se            0\n",
      "symmetry_se                  0\n",
      "fractal_dimension_se         0\n",
      "radius_worst                 0\n",
      "texture_worst                0\n",
      "perimeter_worst              0\n",
      "area_worst                   0\n",
      "smoothness_worst             0\n",
      "compactness_worst            0\n",
      "concavity_worst              0\n",
      "concave points_worst         0\n",
      "symmetry_worst               0\n",
      "fractal_dimension_worst      0\n",
      "Unnamed: 32                569\n",
      "dtype: int64\n",
      "\n",
      "Summary Statistics:\n",
      "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
      "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
      "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
      "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
      "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
      "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
      "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
      "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
      "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
      "\n",
      "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "count       569.000000        569.000000      569.000000           569.000000   \n",
      "mean          0.096360          0.104341        0.088799             0.048919   \n",
      "std           0.014064          0.052813        0.079720             0.038803   \n",
      "min           0.052630          0.019380        0.000000             0.000000   \n",
      "25%           0.086370          0.064920        0.029560             0.020310   \n",
      "50%           0.095870          0.092630        0.061540             0.033500   \n",
      "75%           0.105300          0.130400        0.130700             0.074000   \n",
      "max           0.163400          0.345400        0.426800             0.201200   \n",
      "\n",
      "       symmetry_mean  ...  texture_worst  perimeter_worst   area_worst  \\\n",
      "count     569.000000  ...     569.000000       569.000000   569.000000   \n",
      "mean        0.181162  ...      25.677223       107.261213   880.583128   \n",
      "std         0.027414  ...       6.146258        33.602542   569.356993   \n",
      "min         0.106000  ...      12.020000        50.410000   185.200000   \n",
      "25%         0.161900  ...      21.080000        84.110000   515.300000   \n",
      "50%         0.179200  ...      25.410000        97.660000   686.500000   \n",
      "75%         0.195700  ...      29.720000       125.400000  1084.000000   \n",
      "max         0.304000  ...      49.540000       251.200000  4254.000000   \n",
      "\n",
      "       smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "count        569.000000         569.000000       569.000000   \n",
      "mean           0.132369           0.254265         0.272188   \n",
      "std            0.022832           0.157336         0.208624   \n",
      "min            0.071170           0.027290         0.000000   \n",
      "25%            0.116600           0.147200         0.114500   \n",
      "50%            0.131300           0.211900         0.226700   \n",
      "75%            0.146000           0.339100         0.382900   \n",
      "max            0.222600           1.058000         1.252000   \n",
      "\n",
      "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
      "count            569.000000      569.000000               569.000000   \n",
      "mean               0.114606        0.290076                 0.083946   \n",
      "std                0.065732        0.061867                 0.018061   \n",
      "min                0.000000        0.156500                 0.055040   \n",
      "25%                0.064930        0.250400                 0.071460   \n",
      "50%                0.099930        0.282200                 0.080040   \n",
      "75%                0.161400        0.317900                 0.092080   \n",
      "max                0.291000        0.663800                 0.207500   \n",
      "\n",
      "       Unnamed: 32  \n",
      "count          0.0  \n",
      "mean           NaN  \n",
      "std            NaN  \n",
      "min            NaN  \n",
      "25%            NaN  \n",
      "50%            NaN  \n",
      "75%            NaN  \n",
      "max            NaN  \n",
      "\n",
      "[8 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and Explore the Data\n",
    "# -----------------------------------\n",
    "\n",
    "# Import necessary libraries\n",
    "# We're using pandas for data manipulation and exploration\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'data.csv' with the actual path to your dataset\n",
    "file_path = 'data.csv'  # Example file path, update as needed\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "# This helps us understand the structure, data types, and other metadata\n",
    "print(\"Dataset Information:\")\n",
    "print(data.info())  # Display information about columns, data types, and non-null values\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "# It gives a quick look at the data to understand its structure and content\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data.head())  # Display the first 5 rows to inspect the data\n",
    "\n",
    "# Check for missing values\n",
    "# This helps us identify if there are any missing values in the dataset\n",
    "# It is important to address missing data before proceeding with analysis\n",
    "print(\"\\nMissing Values in the Dataset:\")\n",
    "print(data.isnull().sum())  # Count missing values per column\n",
    "\n",
    "# Display basic statistics for numerical columns\n",
    "# This will give us an overview of the distributions and ranges of numerical values in the dataset\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(data.describe())  # Display basic statistical measures (mean, std, min, max, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c288b3c-5fb1-4352-80e8-46ff77722c10",
   "metadata": {},
   "source": [
    "### Step 2: Clean the Data\n",
    "\n",
    "In this step, we'll clean the dataset by removing unnecessary columns. We'll drop columns like 'id' and 'Unnamed: 32' as they don't provide meaningful information for our analysis. After cleaning, we'll display the updated dataset's basic information and show the first few rows to confirm the cleaning process was successful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de93b4f-7353-40a0-9dd2-10ae015f5f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   diagnosis                569 non-null    object \n",
      " 1   radius_mean              569 non-null    float64\n",
      " 2   texture_mean             569 non-null    float64\n",
      " 3   perimeter_mean           569 non-null    float64\n",
      " 4   area_mean                569 non-null    float64\n",
      " 5   smoothness_mean          569 non-null    float64\n",
      " 6   compactness_mean         569 non-null    float64\n",
      " 7   concavity_mean           569 non-null    float64\n",
      " 8   concave points_mean      569 non-null    float64\n",
      " 9   symmetry_mean            569 non-null    float64\n",
      " 10  fractal_dimension_mean   569 non-null    float64\n",
      " 11  radius_se                569 non-null    float64\n",
      " 12  texture_se               569 non-null    float64\n",
      " 13  perimeter_se             569 non-null    float64\n",
      " 14  area_se                  569 non-null    float64\n",
      " 15  smoothness_se            569 non-null    float64\n",
      " 16  compactness_se           569 non-null    float64\n",
      " 17  concavity_se             569 non-null    float64\n",
      " 18  concave points_se        569 non-null    float64\n",
      " 19  symmetry_se              569 non-null    float64\n",
      " 20  fractal_dimension_se     569 non-null    float64\n",
      " 21  radius_worst             569 non-null    float64\n",
      " 22  texture_worst            569 non-null    float64\n",
      " 23  perimeter_worst          569 non-null    float64\n",
      " 24  area_worst               569 non-null    float64\n",
      " 25  smoothness_worst         569 non-null    float64\n",
      " 26  compactness_worst        569 non-null    float64\n",
      " 27  concavity_worst          569 non-null    float64\n",
      " 28  concave points_worst     569 non-null    float64\n",
      " 29  symmetry_worst           569 non-null    float64\n",
      " 30  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), object(1)\n",
      "memory usage: 137.9+ KB\n",
      "None\n",
      "\n",
      "First 5 Rows of the Cleaned Dataset:\n",
      "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0         M        17.99         10.38          122.80     1001.0   \n",
      "1         M        20.57         17.77          132.90     1326.0   \n",
      "2         M        19.69         21.25          130.00     1203.0   \n",
      "3         M        11.42         20.38           77.58      386.1   \n",
      "4         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
      "0         0.2419  ...         25.38          17.33           184.60   \n",
      "1         0.1812  ...         24.99          23.41           158.80   \n",
      "2         0.2069  ...         23.57          25.53           152.50   \n",
      "3         0.2597  ...         14.91          26.50            98.87   \n",
      "4         0.1809  ...         22.54          16.67           152.20   \n",
      "\n",
      "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Clean the Data\n",
    "# -----------------------\n",
    "\n",
    "# Drop unnecessary columns\n",
    "# We will remove 'id' and 'Unnamed: 32' columns since they don't provide meaningful information for our analysis\n",
    "data_cleaned = data.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "\n",
    "# Display the updated dataset information\n",
    "# After removing the unnecessary columns, we can check the new structure of the dataset\n",
    "print(\"Updated Dataset Information:\")\n",
    "print(data_cleaned.info())  # This will show the updated dataset, confirming the drop\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "# It's helpful to inspect the cleaned data to make sure the columns have been correctly removed\n",
    "print(\"\\nFirst 5 Rows of the Cleaned Dataset:\")\n",
    "print(data_cleaned.head())  # Preview the first 5 rows of the cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0626d-12c1-4eb2-bfdd-a02e935accf6",
   "metadata": {},
   "source": [
    "### Step 3: Encode the Target Variable\n",
    "\n",
    "In this step, we'll encode the target variable 'diagnosis', which represents the class of the cancer (Benign or Malignant), into numerical values. We'll map 'B' to 0 (Benign) and 'M' to 1 (Malignant). After encoding, we'll verify the changes by checking the unique values in the 'diagnosis' column and displaying the first few rows of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd045c2-84fe-422c-9d17-7b0b3e284e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the 'diagnosis' column after encoding:\n",
      "[1 0]\n",
      "\n",
      "First 5 Rows After Encoding:\n",
      "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0          1        17.99         10.38          122.80     1001.0   \n",
      "1          1        20.57         17.77          132.90     1326.0   \n",
      "2          1        19.69         21.25          130.00     1203.0   \n",
      "3          1        11.42         20.38           77.58      386.1   \n",
      "4          1        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
      "0         0.2419  ...         25.38          17.33           184.60   \n",
      "1         0.1812  ...         24.99          23.41           158.80   \n",
      "2         0.2069  ...         23.57          25.53           152.50   \n",
      "3         0.2597  ...         14.91          26.50            98.87   \n",
      "4         0.1809  ...         22.54          16.67           152.20   \n",
      "\n",
      "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Encode the Target Variable\n",
    "# -----------------------\n",
    "\n",
    "# Map the diagnosis column to numerical values\n",
    "# 'B' (Benign) will be mapped to 0, and 'M' (Malignant) will be mapped to 1\n",
    "data_cleaned['diagnosis'] = data_cleaned['diagnosis'].map({'B': 0, 'M': 1})\n",
    "\n",
    "# Check the unique values to confirm encoding\n",
    "# We can inspect the unique values in the 'diagnosis' column to ensure the mapping was done correctly\n",
    "print(\"Unique values in the 'diagnosis' column after encoding:\")\n",
    "print(data_cleaned['diagnosis'].unique())\n",
    "\n",
    "# Display the first few rows to verify changes\n",
    "# After encoding, let's preview the dataset to check the changes in the 'diagnosis' column\n",
    "print(\"\\nFirst 5 Rows After Encoding:\")\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ef79c-ce7f-4560-821d-ae039c868752",
   "metadata": {},
   "source": [
    "### Step 4: Split the Dataset into Training and Testing Sets\n",
    "\n",
    "Now, we will split the dataset into training and testing sets. This is an important step to evaluate the model's performance on unseen data. We'll separate the features (X) from the target variable (y), and then use `train_test_split` from Scikit-learn to create the training and testing sets. We will use a test size of 20% and set a random seed for reproducibility. Finally, we'll check the shape of the training and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68eee671-513c-47d0-80b5-d3ba10f6ef6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Features: (455, 30)\n",
      "Shape of Testing Features: (114, 30)\n",
      "Shape of Training Labels: (455,)\n",
      "Shape of Testing Labels: (114,)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split the Dataset into Training and Testing Sets\n",
    "# -----------------------\n",
    "\n",
    "# Import train_test_split for splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "# The target variable 'diagnosis' is separated from the features\n",
    "X = data_cleaned.drop('diagnosis', axis=1)\n",
    "y = data_cleaned['diagnosis']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# We use 20% of the data for testing, and the rest (80%) for training\n",
    "# The stratify parameter ensures that the target variable's distribution is maintained in both training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shape of the training and testing sets\n",
    "# We print the shape of features and labels for both the training and testing sets\n",
    "print(\"Shape of Training Features:\", X_train.shape)\n",
    "print(\"Shape of Testing Features:\", X_test.shape)\n",
    "print(\"Shape of Training Labels:\", y_train.shape)\n",
    "print(\"Shape of Testing Labels:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24d1a2-eecd-4f07-a5e8-0d5d3cddaf4b",
   "metadata": {},
   "source": [
    "### Step 6: Scale the Features\n",
    "\n",
    "In this step, we will scale the features using `StandardScaler` from Scikit-learn. Feature scaling is important for algorithms that rely on distance metrics or optimization, such as Logistic Regression. We will apply scaling to both the training and testing datasets, re-train the Logistic Regression model on the scaled data, and evaluate its performance. We will display the accuracy, classification report, and confusion matrix for the scaled model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a31ce297-d751-4fb0-a83d-6edde9b142e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy (Scaled): 0.9649122807017544\n",
      "\n",
      "Classification Report (Scaled):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        72\n",
      "           1       0.97      0.93      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "Confusion Matrix (Scaled):\n",
      "[[71  1]\n",
      " [ 3 39]]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Scale the Features\n",
    "# ---------------------------\n",
    "\n",
    "# Import StandardScaler from scikit-learn to scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# Fit and transform the training data, and transform the test data\n",
    "# The 'fit_transform' is applied to the training data, and 'transform' is applied to the test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Re-train the Logistic Regression model on the scaled data\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the scaled test data\n",
    "y_pred_scaled = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance on the scaled data\n",
    "\n",
    "# Calculate the accuracy score of the model on the scaled test set\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(\"Logistic Regression Model Accuracy (Scaled):\", accuracy_scaled)\n",
    "\n",
    "# Display the classification report for the scaled model\n",
    "# The classification report provides precision, recall, and f1-score for each class\n",
    "print(\"\\nClassification Report (Scaled):\")\n",
    "print(classification_report(y_test, y_pred_scaled))\n",
    "\n",
    "# Display the confusion matrix for the scaled model\n",
    "# The confusion matrix helps us understand the true positives, false positives, true negatives, and false negatives\n",
    "print(\"\\nConfusion Matrix (Scaled):\")\n",
    "print(confusion_matrix(y_test, y_pred_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d175c-ea89-48b3-9de6-1b04a22fe7cb",
   "metadata": {},
   "source": [
    "### Step 7: Train the Support Vector Machine (SVM) and Random Forest Models\n",
    "\n",
    "In this step, we will train two additional machine learning models: Support Vector Machine (SVM) and Random Forest. Both models will be trained using the scaled features. We will evaluate their performance by calculating the accuracy of each model and compare them to the Logistic Regression model. The results will be stored in a dictionary for easy comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855c3c73-50ff-412d-8fe5-2b57f7cc5e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracies: {'Logistic Regression': 0.9649122807017544, 'Support Vector Machine': 0.9736842105263158, 'Random Forest': 0.9736842105263158}\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Train the Support Vector Machine (SVM) and Random Forest Models\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# Import SVC for Support Vector Machine and RandomForestClassifier for Random Forest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the Support Vector Machine (SVM)\n",
    "svm = SVC(random_state=42)  # Initialize the SVM model\n",
    "svm.fit(X_train_scaled, y_train)  # Train the SVM model\n",
    "y_pred_svm = svm.predict(X_test_scaled)  # Make predictions on the test set\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)  # Evaluate the accuracy of the SVM model\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)  # Initialize the Random Forest model\n",
    "rf.fit(X_train_scaled, y_train)  # Train the Random Forest model\n",
    "y_pred_rf = rf.predict(X_test_scaled)  # Make predictions on the test set\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)  # Evaluate the accuracy of the Random Forest model\n",
    "\n",
    "# Store the results in a dictionary for easy comparison\n",
    "model_accuracies = {\n",
    "    'Logistic Regression': accuracy_scaled,\n",
    "    'Support Vector Machine': accuracy_svm,\n",
    "    'Random Forest': accuracy_rf\n",
    "}\n",
    "\n",
    "# Print the accuracies for each model\n",
    "print(\"Model Accuracies:\", model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a2928-aea7-4f68-b49b-8c284fb75362",
   "metadata": {},
   "source": [
    "### Step 8: Perform Paired t-test to Compare Model Performance\n",
    "\n",
    "In this step, we will use cross-validation to generate multiple accuracy scores for each model (Logistic Regression, SVM, and Random Forest) and then perform a **paired t-test** to statistically compare the performance of these models. This will help us determine if there is a significant difference in their accuracies.\n",
    "\n",
    "We will:\n",
    "1. Use **cross-validation** to obtain multiple accuracy scores for each model.\n",
    "2. Perform the **paired t-test** to compare the models.\n",
    "3. Determine if the differences are statistically significant based on the p-value (commonly, a p-value < 0.05 indicates a significant difference).\n",
    "\n",
    "Now, let's proceed with the cross-validation and t-test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73bcb563-9733-4c57-b298-b0c8cff91e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test between Logistic Regression and SVM: T-statistic = -1.0, P-value = 0.373900966300059\n",
      "Paired t-test between Logistic Regression and Random Forest: T-statistic = 0.6064784348631235, P-value = 0.5769327973042664\n",
      "The difference in performance between Logistic Regression and SVM is not statistically significant.\n",
      "The difference in performance between Logistic Regression and Random Forest is not statistically significant.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Perform Paired t-test to Compare Model Performance\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Perform cross-validation to get multiple accuracy scores for each model\n",
    "log_reg_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "svm_scores = cross_val_score(svm, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "rf_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform paired t-test between Logistic Regression and SVM\n",
    "t_stat_lr_svm, p_value_lr_svm = ttest_rel(log_reg_scores, svm_scores)\n",
    "\n",
    "# Perform paired t-test between Logistic Regression and Random Forest\n",
    "t_stat_lr_rf, p_value_lr_rf = ttest_rel(log_reg_scores, rf_scores)\n",
    "\n",
    "# Print results for both comparisons\n",
    "print(f\"Paired t-test between Logistic Regression and SVM: T-statistic = {t_stat_lr_svm}, P-value = {p_value_lr_svm}\")\n",
    "print(f\"Paired t-test between Logistic Regression and Random Forest: T-statistic = {t_stat_lr_rf}, P-value = {p_value_lr_rf}\")\n",
    "\n",
    "# Check significance (commonly p-value < 0.05 is considered significant)\n",
    "if p_value_lr_svm < 0.05:\n",
    "    print(\"The difference in performance between Logistic Regression and SVM is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in performance between Logistic Regression and SVM is not statistically significant.\")\n",
    "\n",
    "if p_value_lr_rf < 0.05:\n",
    "    print(\"The difference in performance between Logistic Regression and Random Forest is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in performance between Logistic Regression and Random Forest is not statistically significant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4a2df-db9f-4f49-be99-d162a2af6ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
